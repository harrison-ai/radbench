{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Home","text":""},{"location":"#radbench-radiology-benchmark-framework","title":"RadBench: Radiology Benchmark Framework","text":""},{"location":"#overview","title":"Overview","text":"<p>RadBench is a radiology benchmark framework developed by Harrison.ai. It is designed to evaluate the performance of Harrison.ai's foundational radiology model, <code>harrison.rad.1</code>, against other competitive models in the field. The framework employs a rigorous evaluation methodology across three distinct datasets to ensure the models are thoroughly assessed for clinical relevance, accuracy, and case comprehension. These datasets are:</p> <ol> <li> <p>RadBench Dataset: A new visual question-answering dataset designed by Harrison.ai to benchmark radiology models.</p> </li> <li> <p>VQA-RAD Dataset: A visual question-answering dataset for radiology, available at Nature Datasets.</p> </li> <li> <p>Fellowship of the Royal College of Radiologists (FRCR) 2B Examination: Curated for the Fellowship of the Royal College of Radiologists (FRCR) Rapids 2B exam, obtained from third parties to ensure fairness in our evaluation process.</p> </li> </ol>"},{"location":"readme/","title":"Home","text":""},{"location":"readme/#radbench-radiology-benchmark-framework","title":"RadBench: Radiology Benchmark Framework","text":""},{"location":"readme/#overview","title":"Overview","text":"<p>RadBench is a radiology benchmark framework developed by Harrison.ai. It is designed to evaluate the performance of Harrison.ai's foundational radiology model, <code>harrison.rad.1</code>, against other competitive models in the field. The framework employs a rigorous evaluation methodology across three distinct datasets to ensure the models are thoroughly assessed for clinical relevance, accuracy, and case comprehension. These datasets are:</p> <ol> <li> <p>RadBench Dataset: A new visual question-answering dataset designed by Harrison.ai to benchmark radiology models.</p> </li> <li> <p>VQA-RAD Dataset: A visual question-answering dataset for radiology, available at Nature Datasets.</p> </li> <li> <p>Fellowship of the Royal College of Radiologists (FRCR) 2B Examination: Curated for the Fellowship of the Royal College of Radiologists (FRCR) Rapids 2B exam, obtained from third parties to ensure fairness in our evaluation process.</p> </li> </ol>"},{"location":"datasets/frcr/","title":"FRCR","text":""},{"location":"datasets/frcr/#frcr","title":"FRCR","text":"<p>Medical specialists undertake rigorous and thorough evaluation examinations before practicing Radiology. The Fellowship of the Royal College of Radiologists (FRCR) is one such examination. We used a component of this examination, the FRCR 2B Rapids <sup>1</sup>, to benchmark radiology foundation models.</p> <p>While the actual examinations are kept confidential to prevent leakage, mock FRCR examinations are available on various established educational websites. Our FRCR evaluation dataset is comprised of 70 FRCR examination sheets procured from these established third party organisations. We have sourced this dataset from third party to ensure fairness in our evaluation process.</p> <ol> <li> <p>The Royal College of Radiologists. Frcr part 2b (radiology) - cr2b | the royal college of radiologists. URL: https://www.rcr.ac.uk/exams-training/rcr-exams/clinical-radiology-exams/frcr-part-2b-radiology-cr2b/ (visited on 2024-08-07).\u00a0\u21a9</p> </li> </ol>"},{"location":"datasets/radbench/","title":"RadBench","text":""},{"location":"datasets/radbench/#radbench-dataset","title":"RadBench Dataset","text":"<p>RadBench dataset is collation of clinically relevant Radiology specific visual questions and answers (VQA) based on plain film X-ray. This VQA dataset is clinically comprehensive, covering 3 or more questions per medical imaging. The radiology images for this set are sourced from Medpix and Radiopaedia. RadBench is curated by medical doctors with expertise in relevant fields who interpret these images as part of their clinical duties. </p> <p></p>"},{"location":"datasets/radbench/#overview","title":"Overview","text":"<p>The RadBench dataset is formatted similarly to VQA-Rad<sup>1</sup> to ensure ease of use by Medical/Radiology communities. Some key differences are:</p> <ul> <li>Rich set of possible answers: The closed questions of the RadBench dataset have a set of possible answers explicitly defined.</li> <li>Level of correctness: The set of possible answers for given question is also ordered in terms of relative correctness. This is done to account for the fact that some options can be more incorrect than others. This ordering also helps with differential diagnosis.</li> <li>Multi-turn Questionnaire: Questions are ordered per case by specificity - meaning that if evaluated in the same context, they should be asked in that order. For example, \"Is there a fracture in the study?\" should be asked prior to \"Which side is the fracture on?\" as the second question implies the answer to the first.</li> </ul>"},{"location":"datasets/radbench/#why-radbench","title":"Why RadBench?","text":"<p>There has been a growing concern within computer vision and deep learning (CV &amp; DL) communities that we have started to overfit popular existing benchmarks, such as ImageNet <sup>2</sup>. We share this concern and worry that Radiology foundation models perhaps are also starting to overfit on VQA-Rad <sup>1</sup>. Besides, existing Radiology VQA datasets have several shortcomings:</p> <ul> <li>Some datasets have automatically generated questions and answers from existing noisy labels extracted from radiology reports. This leads to unnatural and ambiguous questions which cannot be adequately answered given the image. For instance:<ul> <li>This question <code>In the given Chest X-Ray, is cardiomegaly present in the upper? (please answer yes/no)</code> (dataset source: ProbMed)<sup>3</sup> is anatomically impossible to answer as cardiomegaly is not divided into <code>upper</code> and <code>lower</code>. </li> <li>Likewise, in SLAKE <sup>4</sup> dataset,  given the image <code>xmlab470/source.jpg</code>, question <code>Where is the brain non-enhancing tumor?</code> is asked. However the image is an axial non-contrast T2 MRI of the brain whereby answering the question of 'non-enhancing tumor' is not possible. The answer for this question is also given as <code>Upper Left Lobe</code> which is not a valid anatomical region in the brain. This should be answered as <code>anterior left frontal lobe</code>. </li> </ul> </li> <li>Some existing datasets have been curated by non-medical specialists, leading to questions which may be less relevant to everyday clinical work and pathology.</li> <li>Existing datasets do not include more than one image per question, whereas in radiology many studies do include more than one view. Having only one image does not allow us to evaluate the model for its ability of comparing multiple images at once, which is a very clinically relevant task.</li> <li>Existing datasets do not specify the context in which the images should be used. This is relevant to RadBench as more than one image can be used in a single question. In RadBench, the <code>&lt;i&gt;</code> token is used to denote the location of an image in relation to the surrounding words (more specifically tokens). This allows specific references to the images in the question e.g. \"the first study\" or \"the second study\". As a result multi-turn comparison questions can now be asked.  </li> <li>Existing datasets are not selected for clinically challenging cases where the pathology is visually subtle or rare. RadBench specifically selects a wide range of pathology in different anatomical parts with the intention of including challenging cases.  </li> </ul>"},{"location":"datasets/radbench/#working-with-radbench-dataset","title":"Working with RadBench Dataset","text":"<p>The RadBench dataset is a collection of 89 unique cases, consisting of 40 cases sourced from MedPix and 49 cases sourced from Radiopaedia. A total of 497 questions were asked for these cases, with 377 questions being closed-ended and 120 questions being open-ended.</p> <p>Here is a breakdown of the dataset's structure:</p> Header Description imageSource Describes the source of the images, indicating whether they are from MedPix or Radiopaedia. CASE_ID For MedPix cases, this field is left blank. For Radiopaedia cases, it contains a numerical value that can be used to search for the case image at Radiopaedia.org using the provided case ID. imageIDs A list of unique IDs for each image, separated by commas. These IDs can be either MedPix image IDs or Radiopaedia image URLs. To obtain URLs for MedPix images, refer to the following section on loading RadBench images. modality Currently, the dataset only includes \"XR - Plain Film\" modality. IMAGE_ORGAN Specifies the organ present in the image, such as ABDOMEN, BRAIN, or PELVIS. PRIMARY_DX Indicates the primary diagnosis for the given case. QUESTION Represents the question to be provided to the AI model. Q_TYPE Specifies the type of question asked, such as Pathology, Radiological View, Body Part, Clinical, Demographic, or Comparison. ANSWER Denotes the expected answer from the model. A_TYPE Indicates whether the question is open-ended or closed-ended. OPTIONS For closed-ended questions, this field contains a list of answer options, such as \"left\" and \"right\". For open-ended questions, this field is left blank."},{"location":"datasets/radbench/#loading-radbench-images","title":"Loading RadBench images","text":"<p>To load the images for the RadBench dataset, you will need to identify the source of the images using the <code>imageSource</code> column provided in the RadBench dataset.</p> <p>If the images are sourced from Radiopaedia, the image URLs can be found in the <code>imageIDs</code> column and can be directly used to fetch the images.</p> <p>For MedPix images, you will need to query the MedPix image metadata first and obtain the images. To query the metadata, you can use the following URL structure: <code>https://medpix.nlm.nih.gov/rest/image.json?imageID=</code>. Simply append the image ID to the end of the URL. The response will be in JSON format, with the key <code>imageURL</code> containing the URL to the actual image.</p>"},{"location":"datasets/radbench/#acknowledgements","title":"Acknowledgements","text":"<p>We thank Medpix and Radiopaedia and their respective editorial teams and contributors specially NIH, Frank Gaillard, Andrew Dixon, and other Radiopaedia.org contributors for creating such a rich library of cases to test radiology expertise. </p> <ol> <li> <p>JJ Lau, S Gayen, Abacha A Ben, and D. Demner-Fushman. A dataset of clinically generated visual questions and answers about radiology images. Scientific Data, 5(1):2052\u20134463, 2018. URL: https://www.nature.com/articles/sdata2018251.\u00a0\u21a9\u21a9</p> </li> <li> <p>Lucas Beyer, Olivier J. H\u00e9naff, Alexander Kolesnikov, Xiaohua Zhai, and A\u00e4ron van den Oord. Are we done with imagenet? CoRR, 2020. URL: https://arxiv.org/abs/2006.07159.\u00a0\u21a9</p> </li> <li> <p>Q. Yan, X. He, X. Yue, and X. E. Wang. Worse than random? an embarrassingly simple probing evaluation of large multimodal models in medical vqa. ArXiv, 2024. URL: https://arxiv.org/abs/2405.20421.\u00a0\u21a9</p> </li> <li> <p>B. Liu, L. Zhan, L. Xu, L. Ma, Y. Yang, and X. Wu. Slake: a semantically-labeled knowledge-enhanced dataset for medical visual question answering. ArXiv, 2021. URL: https://arxiv.org/abs/2102.09542.\u00a0\u21a9</p> </li> </ol>"},{"location":"datasets/vqa-rad/","title":"VQA-Rad","text":""},{"location":"datasets/vqa-rad/#vqa-rad","title":"VQA-Rad","text":"<p>VQA-Rad is a dataset of clinically generated visual questions and answers about radiology images <sup>1</sup>. This dataset can be downloaded from nature dataset or here or alternatively from Hugging Face.</p> <ol> <li> <p>JJ Lau, S Gayen, Abacha A Ben, and D. Demner-Fushman. A dataset of clinically generated visual questions and answers about radiology images. Scientific Data, 5(1):2052\u20134463, 2018. URL: https://www.nature.com/articles/sdata2018251.\u00a0\u21a9</p> </li> </ol>"}]}